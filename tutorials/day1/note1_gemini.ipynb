{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e00e5bc",
   "metadata": {},
   "source": [
    "# Open Jupyter Notebook in Google Colab\n",
    "\n",
    "1. Go to the following URL:\n",
    "https://colab.research.google.com/\n",
    "\n",
    "\n",
    "2. Click on GitHub tab.\n",
    "\n",
    "<img src=\"../assets/colab1.png\" width=\"500\" alt=\"Create API Key button\"/>\n",
    "\n",
    "\n",
    "3. Paste the URL of this repository:\n",
    "\n",
    "https://github.com/oh-scipe/llm-workshop26/tree/main/tutorials/day1\n",
    "\n",
    "<img src=\"../assets/colab2.png\" width=\"500\" alt=\"Create API Key button\"/>\n",
    "\n",
    "4. Select the notebook you want to open."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce96f09",
   "metadata": {},
   "source": [
    "# Google Gemini\n",
    "\n",
    "This workshop notebook covers setup, authentication, text generation, conversations, streaming, and handling different content types.\n",
    "\n",
    "## What are Large Language Models (LLMs)?\n",
    "\n",
    "**Large Language Models** are deep neural networks trained on vast amounts of text data to understand and generate human-like language. They learn statistical patterns, relationships, and structures in language through a process called **unsupervised learning**.\n",
    "\n",
    "### Key Concepts in Language Modeling:\n",
    "\n",
    "1. **Token-based Processing**: Text is broken into tokens (words, subwords, or characters), and the model learns to predict the next token given previous context.\n",
    "\n",
    "2. **Transformers Architecture**: Modern LLMs use the transformer architecture, which employs attention mechanisms to understand relationships between tokens, regardless of their distance in text.\n",
    "\n",
    "3. **Pre-training and Fine-tuning**: Models are first pre-trained on massive datasets, then fine-tuned for specific tasks or aligned with human preferences.\n",
    "\n",
    "4. **Autoregressive Generation**: LLMs generate text one token at a time, using previously generated tokens as context for the next prediction.\n",
    "\n",
    "Google Gemini represents the latest generation of multimodal LLMs, capable of understanding and generating not just text, but also images, audio, and video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ea8af2",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Libraries\n",
    "\n",
    "First, we'll install the Google GenAI SDK and import necessary libraries.\n",
    "\n",
    "The latest recommended package is `google-genai` which is the official Google AI SDK for accessing Gemini models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbac20c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the Google GenAI SDK\n",
    "# Uncomment the line below to install (if not already installed)\n",
    "# !pip install -q -U google-genai\n",
    "\n",
    "# Import required libraries for interacting with the Gemini API\n",
    "import os       # For environment variable access\n",
    "import json     # For handling JSON data\n",
    "from google import genai  # Official Google Generative AI SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed67f6f",
   "metadata": {},
   "source": [
    "## 2. Set Up API Authentication\n",
    "\n",
    "### Authentication in API-based LLMs\n",
    "\n",
    "The Gemini API uses **API key authentication** to identify and authorize your requests. API keys serve multiple purposes:\n",
    "\n",
    "1. **Identity Verification**: Confirms you're an authorized user\n",
    "2. **Usage Tracking**: Monitors your API calls for billing and rate limiting\n",
    "3. **Security**: Prevents unauthorized access to the service\n",
    "4. **Resource Allocation**: Manages quotas and priorities\n",
    "\n",
    "**Best Practices for API Key Security:**\n",
    "- Store keys in environment variables, never hardcode them\n",
    "- Use `.env` files for local development (add to `.gitignore`)\n",
    "- Rotate keys periodically\n",
    "- Use separate keys for development, testing, and production\n",
    "- Revoke compromised keys immediately\n",
    "\n",
    "### Getting Your Google Gemini API Key\n",
    "\n",
    "Follow these steps to obtain your free API key:\n",
    "\n",
    "**Step 1**: Go to Google AI Studio  \n",
    "https://aistudio.google.com/app/api-keys\n",
    "\n",
    "**Step 2**: Click on \"Create API Key\"  \n",
    "<img src=\"../assets/gem1.png\" width=\"250\" alt=\"Create API Key button\"/>\n",
    "\n",
    "**Step 3**: Choose an arbitrary name and click \"Create Key\"  \n",
    "<img src=\"../assets/gem2.png\" width=\"250\" alt=\"Name your API key\"/>\n",
    "\n",
    "**Step 4**: Copy your key securely  \n",
    "<img src=\"../assets/gem3.png\" width=\"250\" alt=\"Copy API key\"/>\n",
    "\n",
    "⚠️ **Important**: Treat your API key like a password. Never share it or commit it to version control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed6f761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste your API key here (get it from https://aistudio.google.com/app/api-keys)\n",
    "api_key = \"\"\n",
    "\n",
    "# Validate that API key is set before proceeding\n",
    "if not api_key:\n",
    "    raise ValueError(\n",
    "        \"API key not found! Please set your Gemini API key.\\n\"\n",
    "        \"Get your key at: https://aistudio.google.com/app/api-keys\\n\"\n",
    "        \"Then set it in the code above or use: export GEMINI_API_KEY='your-api-key'\"\n",
    "    )\n",
    "\n",
    "# Display masked API key for verification (shows only first 4 and last 4 characters)\n",
    "print(f\"API Key found: {api_key[:4]}****{api_key[-4:]}\")\n",
    "\n",
    "client = genai.Client(api_key=api_key) # Initialize the Gemini client with your API key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6ae0c7",
   "metadata": {},
   "source": [
    "## 3. Available Gemini Models\n",
    "\n",
    "Let's explore the available models and their capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4e8bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Fetch list of all available Gemini models from the API\n",
    "    models_pager = client.models.list()\n",
    "    \n",
    "    # Print header for the model list\n",
    "    print(\"Available Gemini Models:\\n\")\n",
    "    print(f\"{'Model Name':<40} {'Description':<50}\")\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    # Iterate through each model and display its information\n",
    "    count = 0\n",
    "    for model in models_pager:\n",
    "        # Extract just the model name (remove the 'models/' prefix)\n",
    "        model_name = model.name.split('/')[-1]\n",
    "        \n",
    "        # Determine model type based on display name\n",
    "        desc = \"Text Generation Model\"\n",
    "        if \"vision\" in model.display_name.lower():\n",
    "            desc = \"Multimodal Model (Text, Image)\"\n",
    "        elif \"flash\" in model.display_name.lower():\n",
    "            desc = \"Fast, Efficient Model\"\n",
    "        \n",
    "        print(f\"{model_name:<40} {desc:<50}\")\n",
    "        count += 1\n",
    "        \n",
    "except Exception as e:\n",
    "    # Handle errors gracefully with helpful troubleshooting information\n",
    "    error_msg = str(e)\n",
    "    print(f\"✗ Error listing models: {error_msg[:200]}\")\n",
    "    print(\"\\nPossible issues:\")\n",
    "    print(\"- Invalid API key\")    \n",
    "    print(\"- Network connectivity problems\")    \n",
    "    print(\"- API service temporarily unavailable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e376e16b",
   "metadata": {},
   "source": [
    "### 3.1 Available Gemini Models (Based on Your API Access)\n",
    "\n",
    "The cell above shows the models currently available in your account. Here's a breakdown of the main models you have access to:\n",
    "\n",
    "| Model Name | Type | Best Use Case |\n",
    "|------------|------|---------------|\n",
    "| **gemini-3-pro-preview** | Preview/Experimental | Latest Gemini 3.0 Pro - complex reasoning, analysis |\n",
    "| **gemini-3-flash-preview** | Preview/Experimental | Latest Gemini 3.0 Flash - fast, efficient, high-volume tasks |\n",
    "| **gemini-3-pro-image-preview** | Multimodal Preview | Image generation and understanding |\n",
    "| **gemma-3-27b-it** | Open Model | Instruction-tuned Gemma model (27B parameters) |\n",
    "| **deep-research-pro-preview-12-2025** | Specialized | Advanced research and analysis tasks |\n",
    "\n",
    "### Model Naming Convention:\n",
    "\n",
    "- **Preview/Experimental** models: Latest features, may change before stable release\n",
    "- **Flash** variants: Optimized for speed and cost-effectiveness\n",
    "- **Pro** variants: Balanced performance for complex tasks\n",
    "- **Image** variants: Support multimodal input/output (images + text)\n",
    "- **Deep Research**: Specialized for in-depth analysis and research tasks\n",
    "- **Gemma**: Google's open-source model family (different from Gemini)\n",
    "\n",
    "### Choosing the Right Model:\n",
    "\n",
    "1. **Fast responses & high volume** → Use `gemini-3-flash-preview`\n",
    "2. **Complex reasoning & analysis** → Use `gemini-3-pro-preview`\n",
    "3. **Image generation/understanding** → Use `gemini-3-pro-image-preview`\n",
    "4. **Research tasks** → Use `deep-research-pro-preview-12-2025`\n",
    "5. **Open-source option** → Use `gemma-3-27b-it`\n",
    "\n",
    "**Important**: Preview models may change or be deprecated. Check the official documentation for the latest stable releases.\n",
    "\n",
    "**Resources:**\n",
    "- **Rate Limits**: https://ai.google.dev/gemini-api/docs/rate-limits  \n",
    "- **Pricing**: https://ai.google.dev/gemini-api/docs/pricing\n",
    "- **Model Documentation**: https://ai.google.dev/gemini-api/docs/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af0cde3",
   "metadata": {},
   "source": [
    "## 4. Generate Text Responses\n",
    "\n",
    "The most basic use of the Gemini API: sending a prompt and getting a text response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a31161",
   "metadata": {},
   "source": [
    "### Language Modeling Fundamentals: Text Generation\n",
    "\n",
    "At its core, **text generation** is the process of predicting the next token given a sequence of previous tokens. This is formalized as:\n",
    "\n",
    "$$P(w_t | w_1, w_2, ..., w_{t-1})$$\n",
    "\n",
    "Where $w_t$ is the token at position $t$, and the model computes the probability distribution over all possible next tokens.\n",
    "\n",
    "**Key Parameters in Generation:**\n",
    "\n",
    "1. **Temperature** ($\\tau$): Controls randomness in token selection. Lower values make the model more deterministic.\n",
    "   $$P_i = \\frac{\\exp(z_i / \\tau)}{\\sum_j \\exp(z_j / \\tau)}$$\n",
    "\n",
    "2. **Top-k Sampling**: Limits selection to the k most probable tokens, preventing unlikely options.\n",
    "\n",
    "3. **Top-p (Nucleus) Sampling**: Selects from the smallest set of tokens whose cumulative probability exceeds p.\n",
    "\n",
    "Let's see this in action with a simple text generation example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955e9b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select which model to use for text generation\n",
    "model = \"gemini-3-flash-preview\"\n",
    "\n",
    "# Define the prompt/question to send to the model\n",
    "prompt = \"Explain quantum computing in 2-3 sentences for someone who is new to the topic.\"\n",
    "    \n",
    "# Display what we're sending to the model\n",
    "print(\"\\033[1m\\033[4mModel:\\033[0m\", model)\n",
    "print(\"\\033[1m\\033[4mPrompt:\\033[0m\", prompt)\n",
    "print()\n",
    "\n",
    "# Generate content using the selected model and prompt\n",
    "response = client.models.generate_content(\n",
    "    model=model,\n",
    "    contents=prompt\n",
    ")\n",
    "\n",
    "# Extract the response parts from the first candidate\n",
    "parts = response.candidates[0].content.parts\n",
    "\n",
    "# Combine all text parts into a single output string\n",
    "text_out = \"\".join(p.text for p in parts if getattr(p, \"text\", None))\n",
    "\n",
    "print(\"\\033[1m\\033[4mResponse:\\033[0m\")\n",
    "print(text_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c474f6",
   "metadata": {},
   "source": [
    "### 4.1 Generation Parameters\n",
    "\n",
    "You can customize the model's behavior using generation parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f68007",
   "metadata": {},
   "source": [
    "### Language Modeling: Controlling Generation with Parameters\n",
    "\n",
    "The way an LLM generates text can be dramatically influenced by **generation parameters**. Understanding these parameters is crucial for controlling the model's creativity, coherence, and reliability.\n",
    "\n",
    "**Temperature**: Think of temperature as a \"creativity knob\"\n",
    "- Low temperature (0.0-0.3): Deterministic, focused on most likely tokens → Good for factual tasks\n",
    "- Medium temperature (0.4-0.7): Balanced creativity → Good for general conversation\n",
    "- High temperature (0.8-1.0+): More random, exploratory → Good for creative writing\n",
    "\n",
    "**Top-k Sampling**: Restricts the model to choosing from only the k most likely next tokens. This prevents the model from selecting improbable words while maintaining diversity.\n",
    "\n",
    "**Top-p (Nucleus) Sampling**: Dynamically adjusts the number of tokens considered by selecting from the smallest set whose cumulative probability exceeds p. This is often more effective than top-k.\n",
    "\n",
    "**Max Output Tokens**: Limits the length of the response, helping to control costs and response time.\n",
    "\n",
    "**Thinking Config**: Controls the model's extended reasoning behavior. Setting `thinking_budget=0` disables the model's internal thinking process, which speeds up responses and reduces token usage for tasks that don't require complex reasoning.\n",
    "\n",
    "Let's experiment with different parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355961b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.genai import types\n",
    "\n",
    "model = \"gemini-2.5-flash\"  # Using a cheaper yet fast model\n",
    "\n",
    "thinking_config=types.ThinkingConfig(thinking_budget=0) # Disable thinking for faster responses\n",
    "\n",
    "# Using generation configuration to customize behavior\n",
    "gen_config = types.GenerateContentConfig(\n",
    "    temperature=0.7,  # 0.0 = deterministic, 1.0 = more random\n",
    "    # top_p=0.95,       # Nucleus sampling parameter\n",
    "    top_k=40,         # Top K sampling parameter\n",
    "    max_output_tokens=512,  # Limit output length\n",
    "    thinking_config=thinking_config,\n",
    ")\n",
    "\n",
    "prompt = \"Write a creative poem about artificial intelligence.\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=model,\n",
    "    contents=prompt,\n",
    "    config=gen_config\n",
    ")\n",
    "\n",
    "print(\"\\033[1m\\033[4mResponse:\\033[0m\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d441322",
   "metadata": {},
   "source": [
    "## 5. Use System Instructions\n",
    "\n",
    "System instructions allow you to set the model's behavior, tone, and constraints for all requests in a conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5056453",
   "metadata": {},
   "source": [
    "### Language Modeling: System Instructions as Context Priming\n",
    "\n",
    "**System instructions** are a powerful technique in modern LLMs that leverages the concept of **prompt engineering** and **in-context learning**.\n",
    "\n",
    "**How It Works:**\n",
    "- System instructions are prepended to every user message in the conversation\n",
    "- They set the \"persona\" or \"role\" of the model, constraining its behavior\n",
    "- The model uses this context to adjust its response style, tone, and content\n",
    "\n",
    "**Why It Matters:**\n",
    "In transformer models, the attention mechanism allows the model to reference the system instruction when generating each token. This creates a form of **soft conditioning** where the instruction guides generation without explicit fine-tuning.\n",
    "\n",
    "**Best Practices for System Instructions:**\n",
    "1. Be specific and clear about the desired behavior\n",
    "2. Include formatting requirements if needed\n",
    "3. Specify constraints (e.g., language level, tone, length)\n",
    "4. Define what the model should and shouldn't do\n",
    "\n",
    "System instructions are especially useful for:\n",
    "- Role-playing scenarios (tutors, assistants, experts)\n",
    "- Consistent formatting of outputs\n",
    "- Domain-specific behavior\n",
    "- Safety and content filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14da46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model to use\n",
    "model = \"gemini-2.5-flash\"\n",
    "\n",
    "# Define system instructions to set the model's persona and behavior\n",
    "# This acts as a persistent context that influences all responses\n",
    "system_instruction = \"\"\"You are a helpful Python programming tutor. \n",
    "- Provide clear, beginner-friendly explanations\n",
    "- Always include code examples\n",
    "- Encourage questions and practice\n",
    "- Use simple language, avoid jargon\"\"\"\n",
    "\n",
    "# User's question\n",
    "prompt = \"How do I read a file in Python?\"\n",
    "\n",
    "# Generate response with system instruction applied\n",
    "response = client.models.generate_content(\n",
    "    model=model,\n",
    "    contents=prompt,\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=system_instruction,  # Apply the tutor persona\n",
    "        max_output_tokens=300,                   # Limit response length\n",
    "        thinking_config=thinking_config,         # Disable thinking tokens\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display the tutor's response\n",
    "print(\"\\033[1m\\033[4mSystem Instruction:\\033[0m Python Programming Tutor\")\n",
    "print(\"\\033[1m\\033[4mPrompt:\\033[0m\", prompt)\n",
    "print()\n",
    "print(\"\\033[1m\\033[4mResponse:\\033[0m\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49624ab0",
   "metadata": {},
   "source": [
    "## 6. Multi-turn Conversations\n",
    "\n",
    "Use the ChatSession class to build interactive conversations that maintain context across multiple turns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76c42eb",
   "metadata": {},
   "source": [
    "### Language Modeling: Context Windows and Conversation History\n",
    "\n",
    "Multi-turn conversations demonstrate a critical concept in LLMs: **context maintenance** and the **attention mechanism**.\n",
    "\n",
    "**Context Window**: The maximum number of tokens (both input and output) that a model can consider at once. Gemini models have context windows ranging from 1M to 2M tokens.\n",
    "\n",
    "**How Conversations Work:**\n",
    "1. Each message (user and assistant) is stored in the conversation history\n",
    "2. When generating a response, the model attends to all previous messages\n",
    "3. The **self-attention mechanism** computes relevance scores between the current token being generated and all previous tokens\n",
    "4. This allows the model to maintain coherence across multiple turns\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "For each position in the sequence, attention is computed as:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Where Q (query), K (key), and V (value) are learned projections of the input embeddings.\n",
    "\n",
    "**Challenges:**\n",
    "- **Context length limitations**: Older models could only handle short conversations\n",
    "- **Attention complexity**: Grows quadratically with sequence length ($O(n^2)$)\n",
    "- **Context management**: Deciding what to keep when approaching limits\n",
    "\n",
    "Let's see how the model maintains context across multiple conversation turns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f5bb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model for the chat session\n",
    "model = \"gemini-2.5-flash\"\n",
    "\n",
    "# Configure the chat session with system instructions and parameters\n",
    "chat_config = types.GenerateContentConfig(\n",
    "    system_instruction=\"You are a friendly and knowledgeable assistant about machine learning.\",\n",
    "    temperature=0.7,                # Balanced creativity for conversation\n",
    "    max_output_tokens=500,          # Allow longer responses\n",
    "    thinking_config=thinking_config,  # Disable thinking tokens\n",
    ")\n",
    "\n",
    "# Initialize a stateful chat session that maintains conversation history\n",
    "chat = client.chats.create(\n",
    "    model=model,\n",
    "    config=chat_config\n",
    ")\n",
    "\n",
    "try:\n",
    "    # First conversation turn: Ask a basic question\n",
    "    message1 = \"What is machine learning?\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\033[1m\\033[4mTURN 1\\033[0m\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\033[1mUser:\\033[0m\", message1)\n",
    "    response1 = chat.send_message(message1)\n",
    "    print(\"\\033[1mAssistant:\\033[0m\", response1.text)\n",
    "    print()\n",
    "    \n",
    "    # Second turn: Follow-up question (context from turn 1 is maintained)\n",
    "    message2 = \"Can you give me a practical example?\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\033[1m\\033[4mTURN 2\\033[0m\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\033[1mUser:\\033[0m\", message2)\n",
    "    response2 = chat.send_message(message2)\n",
    "    print(\"\\033[1mAssistant:\\033[0m\", response2.text)\n",
    "    print()\n",
    "    \n",
    "    # Third turn: More specific follow-up (builds on entire conversation)\n",
    "    message3 = \"How would you apply this to predicting house prices?\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\033[1m\\033[4mTURN 3\\033[0m\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\033[1mUser:\\033[0m\", message3)\n",
    "    response3 = chat.send_message(message3)\n",
    "    print(\"\\033[1mAssistant:\\033[0m\", response3.text)\n",
    "    \n",
    "except Exception as e:\n",
    "    # Handle errors (e.g., rate limits) and explain the concept\n",
    "    print(f\"⚠ Chat error: {str(e)[:150]}\")\n",
    "    print(\"\\nNote: Chat sessions may hit rate limits. Here's how multi-turn conversations work:\")\n",
    "    print(\"\\n1. Create a chat session with a config\")\n",
    "    print(\"\\n2. Send messages using chat.send_message(message)\")\n",
    "    print(\"\\n3. The model maintains context automatically across turns\")\n",
    "    print(\"\\n4. Each response is based on the entire conversation history\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59144cf",
   "metadata": {},
   "source": [
    "## 7. Stream Responses\n",
    "\n",
    "Stream responses in real-time to receive output progressively instead of waiting for the complete response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f0ef85",
   "metadata": {},
   "source": [
    "### Language Modeling: Streaming and Token-by-Token Generation\n",
    "\n",
    "**Streaming** reveals the fundamental nature of autoregressive language models: they generate text one token at a time.\n",
    "\n",
    "**Autoregressive Generation Process:**\n",
    "1. Given input tokens $[t_1, t_2, ..., t_n]$, predict $t_{n+1}$\n",
    "2. Append $t_{n+1}$ to the sequence: $[t_1, t_2, ..., t_n, t_{n+1}]$\n",
    "3. Use the expanded sequence to predict $t_{n+2}$\n",
    "4. Repeat until a stopping condition (max tokens, end-of-sequence token, etc.)\n",
    "\n",
    "**Why Streaming Matters:**\n",
    "- **User Experience**: Users see output immediately instead of waiting for complete response\n",
    "- **Latency**: Reduces perceived response time, especially for long outputs\n",
    "- **Interactivity**: Enables real-time applications like chatbots and code assistants\n",
    "- **Debugging**: Helps understand the model's generation process\n",
    "\n",
    "**Technical Implementation:**\n",
    "Instead of waiting for the entire forward pass to complete, the API sends each generated token (or small chunks) as they're produced. This requires maintaining the model state across multiple network responses.\n",
    "\n",
    "**Trade-offs:**\n",
    "- More network overhead (multiple requests vs. one)\n",
    "- Slightly higher computational cost\n",
    "- Better user experience for long responses\n",
    "\n",
    "Let's observe streaming in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fea7d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # For measuring response timing\n",
    "\n",
    "# Configure the model and prompt\n",
    "model = \"gemini-2.5-flash\"\n",
    "prompt = \"Write a story about a robot learning to dance. EXACTLY 7 sentences. No line breaks.\"\n",
    "\n",
    "# Set generation parameters for creative output\n",
    "config = types.GenerateContentConfig(\n",
    "    max_output_tokens=1000,          # Allow long story\n",
    "    temperature=0.9,                  # High creativity for storytelling\n",
    "    thinking_config=thinking_config,  # Disable thinking tokens\n",
    ")\n",
    "\n",
    "print(\"\\033[1m\\033[4mStreaming Response:\\033[0m\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Track timing information\n",
    "t0 = time.time()\n",
    "chunk_count = 0\n",
    "\n",
    "# Stream the response token-by-token (or in small chunks)\n",
    "for chunk in client.models.generate_content_stream(\n",
    "    model=model,\n",
    "    contents=prompt,\n",
    "    config=config,\n",
    "):\n",
    "    chunk_count += 1\n",
    "    dt = time.time() - t0  # Time elapsed since start\n",
    "\n",
    "    # Extract text from the current chunk\n",
    "    text_piece = chunk.text or \"\"\n",
    "\n",
    "    # Display chunk metadata and content as it arrives\n",
    "    print(f\"\\n[Chunk {chunk_count} @ {dt:.3f}s | {len(text_piece)} chars]\")\n",
    "    if text_piece:\n",
    "        print(text_piece, end=\"\", flush=True)  # Print without newline, flush immediately"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e882132d",
   "metadata": {},
   "source": [
    "## 8. Working with Different Multi-modal Contents\n",
    "\n",
    "Gemini can handle multiple content types including text, images, and files. Let's demonstrate with text and image examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05082570",
   "metadata": {},
   "source": [
    "### Language Modeling: Multimodal Understanding\n",
    "\n",
    "Modern LLMs like Gemini are **multimodal**, meaning they can process and understand multiple types of data: text, images, audio, and video.\n",
    "\n",
    "**How Multimodal Models Work:**\n",
    "1. **Unified Embedding Space**: Different modalities (text, images) are projected into a shared vector space\n",
    "2. **Cross-modal Attention**: The model learns to attend to relevant features across modalities\n",
    "3. **Joint Training**: Models are trained on paired data (e.g., image-caption pairs) to learn alignments\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Image → Image Encoder (Vision Transformer) → Embeddings ─┐\n",
    "                                                          ├→ Unified Transformer → Output\n",
    "Text → Text Tokenizer → Embeddings ─────────────────────┘\n",
    "```\n",
    "\n",
    "**Applications:**\n",
    "- Image captioning and description\n",
    "- Visual question answering (VQA)\n",
    "- Document understanding (OCR + comprehension)\n",
    "- Code analysis from screenshots\n",
    "- Meme interpretation\n",
    "\n",
    "**Why This Matters:**\n",
    "Traditional text-only LLMs are limited to linguistic information. Multimodal models can:\n",
    "- Understand visual context\n",
    "- Ground language in perceptual information\n",
    "- Bridge the gap between symbolic (text) and perceptual (image) representations\n",
    "\n",
    "Let's explore both text-based code analysis and visual understanding:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2534d194",
   "metadata": {},
   "source": [
    "### 8.1 Text with Code Blocks\n",
    "\n",
    "Gemini can analyze code and provide feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ee9e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gemini-2.5-flash\"\n",
    "\n",
    "# Sample code with intentional issues for the model to analyze\n",
    "code_to_analyze = \"\"\"\n",
    "def fibonacci(n):\n",
    "if n <= 1:\n",
    "    return n\n",
    "return fibonacci(n-1) + fibonacci(n-2)\n",
    "\n",
    "result = fibonacci(10)\n",
    "print(result)\n",
    "\"\"\"\n",
    "\n",
    "# Create a structured prompt for code review\n",
    "prompt = f\"\"\"Review this Python code and suggest improvements:\n",
    "\n",
    "```python\n",
    "{code_to_analyze}\n",
    "```\n",
    "\n",
    "Focus on:\n",
    "1. Performance issues\n",
    "2. Code clarity\n",
    "3. Best practices\"\"\"\n",
    "\n",
    "# Generate code review and suggestions\n",
    "response = client.models.generate_content(\n",
    "    model=model,\n",
    "    contents=prompt,\n",
    ")\n",
    "\n",
    "# Display the AI-generated code review\n",
    "print(\"\\033[1m\\033[4mCode Review:\\033[0m\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0291ff9",
   "metadata": {},
   "source": [
    "### 8.2 Working with Images\n",
    "\n",
    "Gemini's multimodal capabilities allow it to analyze and understand images alongside text. This example demonstrates **visual question answering (VQA)** - providing an image and a text prompt to get a description or answer.\n",
    "\n",
    "**How Image Analysis Works:**\n",
    "1. **Download/Load Image**: Images can come from URLs, local files, or be generated\n",
    "2. **Convert to Bytes**: The image is converted to raw bytes format\n",
    "3. **Create Part Object**: Using `types.Part.from_bytes()`, we wrap the image data with its MIME type\n",
    "4. **Send Combined Request**: Both text prompt and image are sent together to the model\n",
    "5. **Receive Analysis**: The model processes both modalities and returns text describing the image\n",
    "\n",
    "**Supported Image Formats:**\n",
    "- PNG, JPEG, WebP, HEIC, HEIF\n",
    "- Maximum file size: 20MB (varies by model)\n",
    "- Images are automatically resized if needed\n",
    "\n",
    "**Use Cases:**\n",
    "- Logo/brand identification\n",
    "- Product recognition and description\n",
    "- Document OCR and understanding\n",
    "- Scene description for accessibility\n",
    "- Medical image analysis\n",
    "- Quality inspection\n",
    "\n",
    "⚠️ **Important Note**: The `gemini-3-flash-preview` model used in this example is **NOT** available in the free tier. If you encounter quota errors or want to experiment with advanced models, you'll need to:\n",
    "1. Go to [Google AI Studio](https://aistudio.google.com/)\n",
    "2. Navigate to **Billing** settings\n",
    "3. Add a payment method and upgrade your account\n",
    "4. Check the [Pricing page](https://ai.google.dev/gemini-api/docs/pricing) for current rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc9b4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import requests  # For downloading images from URLs\n",
    "from PIL import Image  # For displaying images\n",
    "from io import BytesIO\n",
    "\n",
    "# Select a multimodal model that can process both text and images\n",
    "model = \"gemini-3-flash-preview\"\n",
    "\n",
    "# URL of the image to analyze\n",
    "image_url = \"https://www.google.com/images/branding/googlelogo/2x/googlelogo_color_272x92dp.png\"\n",
    "prompt = \"What is this logo? Describe it briefly.\"\n",
    "\n",
    "# Download the image as bytes\n",
    "image_bytes = requests.get(image_url, timeout=30).content\n",
    "\n",
    "# Display the image in the notebook\n",
    "print(\"\\033[1m\\033[4mImage Being Analyzed:\\033[0m\")\n",
    "image = Image.open(BytesIO(image_bytes))\n",
    "display(image)\n",
    "print()\n",
    "\n",
    "# Create a Part object from the image bytes\n",
    "image_part = types.Part.from_bytes(\n",
    "    data=image_bytes,\n",
    "    mime_type=\"image/png\",  # Specify the image format\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Send both text prompt and image to the model\n",
    "    response = client.models.generate_content(\n",
    "        model=model,\n",
    "        contents=[prompt, image_part],  # Order can be: [prompt, image] or [image, prompt]\n",
    "    )\n",
    "    # Display the model's description of the image\n",
    "    print(\"\\033[1m\\033[4mImage Analysis Result:\\033[0m\")\n",
    "    print(response.text)\n",
    "except Exception as e:\n",
    "    # Handle any errors during image analysis\n",
    "    print(\"\\033[1m\\033[4mError:\\033[0m\")\n",
    "    print(repr(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e873a40",
   "metadata": {},
   "source": [
    "### 8.3 Image Generation\n",
    "\n",
    "This block demonstrates **text-to-image generation** using Gemini's multimodal output capabilities. This feature allows you to request both text descriptions and actual generated images in a single API call.\n",
    "\n",
    "**How Image Generation Works:**\n",
    "1. **Text Prompt**: Provide a detailed description of the image you want to generate\n",
    "2. **Specify Response Modalities**: Set `response_modalities=[types.Modality.TEXT, types.Modality.IMAGE]`\n",
    "3. **Model Processing**: The model generates both a text response and image data\n",
    "4. **Extract Image Data**: Parse the response to find the inline image data\n",
    "5. **Decode & Display**: Convert base64 data to an actual image file\n",
    "\n",
    "**Best Practices for Prompts:**\n",
    "- Be specific about style (e.g., \"photorealistic\", \"watercolor\", \"studio ghibli style\")\n",
    "- Include details about composition, lighting, and mood\n",
    "- Mention technical aspects (e.g., \"ultra-detailed\", \"4K\", \"cinematic\")\n",
    "- Specify subject and background clearly\n",
    "\n",
    "**Image Generation Parameters:**\n",
    "- **Format**: Images are returned as base64-encoded data\n",
    "- **Quality**: Depends on model and prompt specificity\n",
    "- **Size**: Typically 1024x1024 or similar standard sizes\n",
    "- **Speed**: May take longer than text-only generation\n",
    "\n",
    "⚠️ **Important Note**: Again, the `gemini-3-pro-image-preview` model used in this example is **NOT** available in the free tier. If you encounter quota errors or want to experiment with advanced models, you'll need to:\n",
    "1. Go to [Google AI Studio](https://aistudio.google.com/)\n",
    "2. Navigate to **Billing** settings\n",
    "3. Add a payment method and upgrade your account\n",
    "4. Check the [Pricing page](https://ai.google.dev/gemini-api/docs/pricing) for current rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e0a144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64              # For decoding base64-encoded image data\n",
    "\n",
    "# Select a model that supports image generation\n",
    "model_id = \"gemini-3-pro-image-preview\"  # Must use image-capable model\n",
    "\n",
    "# Describe the image you want to generate\n",
    "prompt_text = \"A futuristic city skyline at sunset, ultra-detailed digital art, in studio ghibli style\"\n",
    "\n",
    "try:\n",
    "    # Generate both text description and the actual image\n",
    "    response = client.models.generate_content(\n",
    "        model=model_id,\n",
    "        contents=prompt_text,\n",
    "        config=types.GenerateContentConfig(\n",
    "            # Request both text and image in the response\n",
    "            response_modalities=[types.Modality.TEXT, types.Modality.IMAGE]\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Display any text description the model provides\n",
    "    if response.text:\n",
    "        print(\"\\033[1m\\033[4mText Output:\\033[0m\")\n",
    "        print(response.text)\n",
    "        print()\n",
    "\n",
    "    # Extract the image from the response parts\n",
    "    image_data = None\n",
    "    for part in response.candidates[0].content.parts:\n",
    "        # Try multiple possible locations for image data\n",
    "        if hasattr(part, 'inline_data') and part.inline_data:\n",
    "            if hasattr(part.inline_data, 'data'):\n",
    "                image_data = part.inline_data.data\n",
    "                break\n",
    "        elif hasattr(part, 'data'):\n",
    "            image_data = part.data\n",
    "            break\n",
    "\n",
    "    if image_data:\n",
    "        # Check if data is already bytes or needs base64 decoding\n",
    "        if isinstance(image_data, bytes):\n",
    "            image_bytes = image_data\n",
    "        else:\n",
    "            image_bytes = base64.b64decode(image_data)\n",
    "        \n",
    "        # Convert bytes to PIL Image\n",
    "        image = Image.open(BytesIO(image_bytes))\n",
    "        \n",
    "        # Save to file\n",
    "        image.save(\"gemini_generated.png\")\n",
    "        print(\"\\033[1m\\033[4mImage Status:\\033[0m Saved as 'gemini_generated.png'\")\n",
    "        print()\n",
    "        \n",
    "        # Display the image in the notebook\n",
    "        print(\"\\033[1m\\033[4mGenerated Image:\\033[0m\")\n",
    "        display(image)\n",
    "    else:\n",
    "        print(\"\\033[1m\\033[4mNote:\\033[0m Image generation may not be available for this model.\")\n",
    "        print(\"The model returned text only. Try using the model for text-to-image tasks via the web interface.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(\"\\033[1m\\033[4mError:\\033[0m\", str(e))\n",
    "    print(\"\\nImage generation might not be available in the current API version.\")\n",
    "    print(\"The gemini-3-pro-image-preview model is primarily for image understanding, not generation.\")\n",
    "    print(\"For image generation, consider using dedicated image generation APIs like Imagen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03c692a",
   "metadata": {},
   "source": [
    "## 9. Other Common Use Cases\n",
    "\n",
    "Here are some practical examples of common use cases:\n",
    "\n",
    "### 9.1 Content Summarization, Data Extraction, Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752dfe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE 1: Content Summarization\n",
    "print(\"\\033[1m\" + \"=\"*70 + \"\\033[0m\")\n",
    "print(\"\\033[1m\\033[4mEXAMPLE 1: Content Summarization\\033[0m\")\n",
    "print(\"\\033[1m\" + \"=\"*70 + \"\\033[0m\")\n",
    "\n",
    "# Sample article to summarize\n",
    "article = \"\"\"\n",
    "Machine learning is transforming how we solve complex problems.\n",
    "From healthcare to finance, AI applications are improving efficiency\n",
    "and enabling new discoveries. Recent breakthroughs in deep learning\n",
    "have made systems capable of understanding language and images.\n",
    "However, challenges remain in interpretability and bias detection.\n",
    "\"\"\"\n",
    "\n",
    "# Generate a concise summary\n",
    "response = client.models.generate_content(\n",
    "    model = \"gemini-3-flash-preview\",  # Fast model for summarization\n",
    "    contents=f\"Summarize this article in 2 sentences:\\n\\n{article}\",\n",
    "    config=types.GenerateContentConfig(max_output_tokens=100, thinking_config=thinking_config,)\n",
    ")\n",
    "print(\"\\033[1m\\033[4mSummary:\\033[0m\")\n",
    "print(response.text)\n",
    "\n",
    "# EXAMPLE 2: Data Extraction\n",
    "print(\"\\n\" + \"\\033[1m\" + \"=\"*70 + \"\\033[0m\")\n",
    "print(\"\\033[1m\\033[4mEXAMPLE 2: Data Extraction\\033[0m\")\n",
    "print(\"\\033[1m\" + \"=\"*70 + \"\\033[0m\")\n",
    "\n",
    "# Unstructured text containing information to extract\n",
    "text = \"John Smith, age 32, works at TechCorp. Contact: john@example.com\"\n",
    "\n",
    "# Extract structured data as JSON\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=f\"\"\"Extract structured data from this text and return as JSON:\n",
    "    \n",
    "{text}\n",
    "\n",
    "Expected fields: name, age, company, email\"\"\",\n",
    "    config=types.GenerateContentConfig(max_output_tokens=200, thinking_config=thinking_config,)\n",
    ")\n",
    "print(\"\\033[1m\\033[4mExtracted Data:\\033[0m\")\n",
    "print(response.text)\n",
    "\n",
    "# EXAMPLE 3: Question Answering\n",
    "print(\"\\n\" + \"\\033[1m\" + \"=\"*70 + \"\\033[0m\")\n",
    "print(\"\\033[1m\\033[4mEXAMPLE 3: Question Answering\\033[0m\")\n",
    "print(\"\\033[1m\" + \"=\"*70 + \"\\033[0m\")\n",
    "\n",
    "# Context document for answering questions\n",
    "context = \"\"\"\n",
    "The Great Wall of China is approximately 13,171 miles long.\n",
    "It was built over many centuries to protect against invasions.\n",
    "Construction began as early as the 7th century BC.\n",
    "\"\"\"\n",
    "\n",
    "# Answer a question based on the provided context\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=f\"\"\"Answer the question based on the context:\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: How long is the Great Wall of China?\"\"\",\n",
    "    config=types.GenerateContentConfig(max_output_tokens=100, thinking_config=thinking_config,)\n",
    ")\n",
    "print(\"\\033[1m\\033[4mAnswer:\\033[0m\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3eb0f7",
   "metadata": {},
   "source": [
    "### 9.2 Building a Mini Q&A System with Context\n",
    "\n",
    "Let's build a simple question-answering system that demonstrates retrieval-augmented generation (RAG) concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac3ba69",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gemini-2.5-flash\"\n",
    "\n",
    "# Compare different prompting strategies for the same task\n",
    "task = \"Explain machine learning\"\n",
    "\n",
    "prompting_strategies = {\n",
    "    \"Vague\": \"Explain machine learning\",\n",
    "    \n",
    "    \"Specific with audience\": \"Explain machine learning to a 10-year-old child using simple analogies\",\n",
    "    \n",
    "    \"Structured output\": \"\"\"Explain machine learning with the following structure:\n",
    "    1. Definition (1 sentence)\n",
    "    2. How it works (2 sentences)\n",
    "    3. Real-world example (1 sentence)\n",
    "    4. Why it matters (1 sentence)\"\"\",\n",
    "    \n",
    "    \"With constraints\": \"\"\"Explain machine learning in exactly 4 sentences. \n",
    "    Use the analogy of teaching a pet a trick. \n",
    "    Avoid technical jargon.\"\"\"\n",
    "}\n",
    "\n",
    "print(\"\\033[1m\" + \"=\"*70 + \"\\033[0m\")\n",
    "print(\"\\033[1m\\033[4mPROMPT ENGINEERING: Strategy Comparison\\033[0m\")\n",
    "print(\"\\033[1m\" + \"=\"*70 + \"\\033[0m\")\n",
    "\n",
    "for strategy_name, prompt in prompting_strategies.items():\n",
    "    response = client.models.generate_content(\n",
    "        model=model,\n",
    "        contents=prompt,\n",
    "        config=types.GenerateContentConfig(\n",
    "            temperature=0.5,\n",
    "            max_output_tokens=100,\n",
    "            thinking_config=thinking_config,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"\\033[1m\\033[4mSTRATEGY: {strategy_name}\\033[0m\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\033[1mPrompt:\\033[0m {prompt[:80]}...\" if len(prompt) > 80 else f\"\\033[1mPrompt:\\033[0m {prompt}\")\n",
    "    print(f\"\\n\\033[1mResponse:\\033[0m\")\n",
    "    print(response.text)\n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Notice how different prompting strategies yield different results!\")\n",
    "print(\"Key lessons:\")\n",
    "print(\"- Specificity improves relevance\")\n",
    "print(\"- Audience targeting adjusts complexity\")\n",
    "print(\"- Structure ensures completeness\")\n",
    "print(\"- Constraints control format and style\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734a4742",
   "metadata": {},
   "source": [
    "### 9.3 Chain-of-Thought Reasoning\n",
    "\n",
    "Demonstrates how prompting the model to think step-by-step improves accuracy on reasoning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5a1fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gemini-2.5-flash\"\n",
    "\n",
    "# Complex reasoning problem\n",
    "problem = \"\"\"A farmer has 17 sheep, and all but 9 die. How many are left?\n",
    "\n",
    "Let's approach this step-by-step:\n",
    "1. Carefully read and understand what \"all but 9\" means\n",
    "2. Break down the problem\n",
    "3. Calculate the answer\n",
    "4. Verify our reasoning\n",
    "\n",
    "Think through this carefully before answering.\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=model,\n",
    "    contents=problem,\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=0.3,\n",
    "        max_output_tokens=400,\n",
    "        thinking_config=thinking_config,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\033[1m\" + \"=\"*70 + \"\\033[0m\")\n",
    "print(\"\\033[1m\\033[4mCHAIN-OF-THOUGHT REASONING\\033[0m\")\n",
    "print(\"\\033[1m\" + \"=\"*70 + \"\\033[0m\")\n",
    "print(f\"\\033[1mProblem:\\033[0m {problem.split('Let')[0].strip()}\")\n",
    "print()\n",
    "print(\"\\033[1mModel's Reasoning:\\033[0m\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b46272",
   "metadata": {},
   "source": [
    "### 9.4 Creative Writing: Story Generation with Style Transfer\n",
    "\n",
    "LLMs can generate creative content in various styles by learning from the patterns and structures in their training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd198213",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gemini-2.5-flash\"\n",
    "\n",
    "# Story prompt with style variations\n",
    "base_story = \"A programmer discovers their code has become sentient.\"\n",
    "\n",
    "styles = [\n",
    "    \"Shakespearean drama\",\n",
    "    \"Hard science fiction\",\n",
    "    \"Children's picture book\"\n",
    "]\n",
    "\n",
    "print(\"\\033[1m\" + \"=\"*70 + \"\\033[0m\")\n",
    "print(\"\\033[1m\\033[4mCREATIVE WRITING: Style Transfer\\033[0m\")\n",
    "print(\"\\033[1m\" + \"=\"*70 + \"\\033[0m\")\n",
    "print(f\"\\033[1mBase Story Premise:\\033[0m {base_story}\")\n",
    "print()\n",
    "\n",
    "for idx, style in enumerate(styles, 1):\n",
    "    prompt = f\"\"\"Write a short story (3-4 paragraphs) based on this premise:\n",
    "\"{base_story}\"\n",
    "\n",
    "Write it in the style of: {style}\n",
    "\n",
    "Make it engaging and authentic to that style.\"\"\"\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=model,\n",
    "        contents=prompt,\n",
    "        config=types.GenerateContentConfig(\n",
    "            temperature=0.9,  # High temperature for creativity\n",
    "            max_output_tokens=500,\n",
    "            thinking_config=thinking_config,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"\\033[1m\\033[4mSTYLE {idx}: {style.upper()}\\033[0m\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(response.text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4655b7f",
   "metadata": {},
   "source": [
    "### 9.5 Text Classification and Sentiment Analysis\n",
    "\n",
    "LLMs excel at **zero-shot** and **few-shot learning**, where they can perform tasks without explicit training, leveraging their broad pre-training.\n",
    "\n",
    "**Zero-shot Learning**: Performing a task with just a description, no examples.\n",
    "**Few-shot Learning**: Performing a task with a handful of examples in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defb6ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gemini-2.5-flash\"\n",
    "\n",
    "# Zero-shot sentiment analysis\n",
    "reviews = [\n",
    "    \"This product exceeded my expectations! Absolutely love it.\",\n",
    "    \"Terrible quality, broke after one day. Do not buy.\",\n",
    "    \"It's okay, nothing special but gets the job done.\",\n",
    "    \"Best purchase I've made this year! Highly recommend.\"\n",
    "]\n",
    "\n",
    "prompt = \"\"\"Analyze the sentiment of these customer reviews and classify each as POSITIVE, NEGATIVE, or NEUTRAL.\n",
    "Also provide a confidence score (0-1) and a brief reason.\n",
    "\n",
    "Format as JSON array with fields: review, sentiment, confidence, reason\n",
    "\n",
    "Reviews:\n",
    "\"\"\"\n",
    "\n",
    "for i, review in enumerate(reviews, 1):\n",
    "    prompt += f\"{i}. {review}\\n\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=model,\n",
    "    contents=prompt,\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=0.1,  # Low temperature for consistent classification\n",
    "        max_output_tokens=800,\n",
    "        thinking_config=thinking_config,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\033[1m\" + \"=\"*70 + \"\\033[0m\")\n",
    "print(\"\\033[1m\\033[4mSentiment Analysis Results:\\033[0m\")\n",
    "print(\"\\033[1m\" + \"=\"*70 + \"\\033[0m\")\n",
    "print(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
