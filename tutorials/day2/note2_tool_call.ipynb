{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "639f3f8a",
      "metadata": {
        "id": "639f3f8a"
      },
      "source": [
        "# Tool Use in Large Language Models\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Welcome to this hands-on tutorial on **Tool Use** (also known as **Function Calling**) in Large Language Models (LLMs)!\n",
        "\n",
        "### What is Tool Use?\n",
        "\n",
        "Tool Use is a powerful feature that allows LLMs to interact with external tools and functions. Instead of trying to perform complex calculations or access real-time information directly, the model can:\n",
        "\n",
        "1. **Recognize** when it needs to use a tool\n",
        "2. **Request** a function call with appropriate parameters\n",
        "3. **Process** the function's output\n",
        "4. **Generate** a natural language response based on the results\n",
        "\n",
        "### Why is Tool Use Important?\n",
        "\n",
        "Tool use bridges these gaps by enabling LLMs to:\n",
        "-  Delegate tasks to specialized tools\n",
        "-  Access up-to-date information\n",
        "-  Perform reliable computations\n",
        "-  Integrate with existing software systems\n",
        "\n",
        "### How Does It Work?\n",
        "\n",
        "The typical function calling workflow consists of:\n",
        "\n",
        "1. **Tool Definition**: Define functions with clear signatures and docstrings\n",
        "2. **Schema Generation**: Convert functions to JSON schemas\n",
        "3. **Model Prompting**: Pass the schemas to the model along with user queries\n",
        "4. **Tool Call Detection**: Parse the model's output for tool call requests\n",
        "5. **Execution**: Execute the requested functions with provided arguments\n",
        "6. **Result Integration**: Feed results back to the model\n",
        "7. **Final Response**: Generate a user-friendly answer\n",
        "\n",
        "In this tutorial, we'll build a **calculator assistant** that can solve complex mathematical expressions by calling appropriate calculator functions.\n",
        "\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56b3f094",
      "metadata": {
        "id": "56b3f094"
      },
      "source": [
        "## Step 1: Import Required Libraries\n",
        "\n",
        "Let's start by importing the necessary libraries:\n",
        "- `json`: For handling JSON data\n",
        "- `re`: For parsing model outputs with regular expressions\n",
        "- `get_json_schema`: A utility from transformers to convert Python functions to JSON schemas\n",
        "\n",
        "**Note: No installation required**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af15dd6e",
      "metadata": {
        "id": "af15dd6e"
      },
      "outputs": [],
      "source": [
        "from typing import Any, Dict, List\n",
        "import json\n",
        "import re\n",
        "from transformers.utils import get_json_schema"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "346e6ee3",
      "metadata": {
        "id": "346e6ee3"
      },
      "source": [
        "## Step 2: Define Calculator Functions\n",
        "\n",
        "### Writing Tool-Ready Functions\n",
        "\n",
        "For a function to work with LLM tool calling, it must follow specific conventions:\n",
        "\n",
        "#### âœ… Required Format:\n",
        "1. **Descriptive function name**: Clear and self-explanatory\n",
        "2. **Type hints**: All arguments must have type annotations\n",
        "3. **Google-style docstring**: Including description and `Args:` section\n",
        "4. **No types in Args section**: Types go in the function signature only\n",
        "\n",
        "#### Example Format:\n",
        "```python\n",
        "def function_name(arg1: type, arg2: type) -> return_type:\n",
        "    \"\"\"Brief description of what the function does.\n",
        "    \n",
        "    Args:\n",
        "        arg1: Description of first argument\n",
        "        arg2: Description of second argument\n",
        "    \n",
        "    Returns:\n",
        "        Description of return value\n",
        "    \"\"\"\n",
        "    # Function implementation\n",
        "```\n",
        "\n",
        "### Our Calculator Functions\n",
        "\n",
        "Below we define 8 calculator functions that our LLM can call:\n",
        "- **add**: Addition of two numbers\n",
        "- **subtract**: Subtraction\n",
        "- **multiply**: Multiplication\n",
        "- **divide**: Division (with zero-check)\n",
        "- **power**: Exponentiation\n",
        "- **square_root**: Square root calculation (with negative-check)\n",
        "- **modulo**: Remainder operation\n",
        "- **absolute_value**: Absolute value\n",
        "\n",
        "Each function returns a dictionary with:\n",
        "- `operation`: The operation performed\n",
        "- `operands`: The input values\n",
        "- `result`: The computed result\n",
        "- `expression`: A human-readable expression\n",
        "- `error`: Error message (if applicable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fead5aa2",
      "metadata": {
        "id": "fead5aa2"
      },
      "outputs": [],
      "source": [
        "# Calculator\n",
        "\n",
        "def add(a: float, b: float) -> Dict[str, Any]:\n",
        "    \"\"\"Add two numbers together.\n",
        "\n",
        "    Args:\n",
        "        a: The first number to add.\n",
        "        b: The second number to add.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the operation name, operands, result, and\n",
        "        expression string.\n",
        "    \"\"\"\n",
        "    result = a + b\n",
        "    return {\n",
        "        \"operation\": \"add\",\n",
        "        \"operands\": [a, b],\n",
        "        \"result\": result,\n",
        "        \"expression\": f\"{a} + {b} = {result}\",\n",
        "    }\n",
        "\n",
        "\n",
        "def subtract(a: float, b: float) -> Dict[str, Any]:\n",
        "    \"\"\"Subtract the second number from the first number.\n",
        "\n",
        "    Args:\n",
        "        a: The minuend (number being subtracted from).\n",
        "        b: The subtrahend (number to subtract).\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the operation name, operands, result, and\n",
        "        expression string.\n",
        "    \"\"\"\n",
        "    result = a - b\n",
        "    return {\n",
        "        \"operation\": \"subtract\",\n",
        "        \"operands\": [a, b],\n",
        "        \"result\": result,\n",
        "        \"expression\": f\"{a} - {b} = {result}\",\n",
        "    }\n",
        "\n",
        "\n",
        "def multiply(a: float, b: float) -> Dict[str, Any]:\n",
        "    \"\"\"Multiply two numbers together.\n",
        "\n",
        "    Args:\n",
        "        a: The first factor.\n",
        "        b: The second factor.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the operation name, operands, result, and\n",
        "        expression string.\n",
        "    \"\"\"\n",
        "    result = a * b\n",
        "    return {\n",
        "        \"operation\": \"multiply\",\n",
        "        \"operands\": [a, b],\n",
        "        \"result\": result,\n",
        "        \"expression\": f\"{a} * {b} = {result}\",\n",
        "    }\n",
        "\n",
        "\n",
        "def divide(a: float, b: float) -> Dict[str, Any]:\n",
        "    \"\"\"Divide the first number by the second number.\n",
        "\n",
        "    Args:\n",
        "        a: The dividend (numerator).\n",
        "        b: The divisor (denominator). Must not be zero.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the operation name, operands, result,\n",
        "        expression string, and an error field if division by zero occurs.\n",
        "    \"\"\"\n",
        "    if b == 0:\n",
        "        return {\n",
        "            \"operation\": \"divide\",\n",
        "            \"operands\": [a, b],\n",
        "            \"result\": None,\n",
        "            \"error\": \"Division by zero is undefined\",\n",
        "            \"expression\": f\"{a} / {b} = ERROR\",\n",
        "        }\n",
        "    result = a / b\n",
        "    return {\n",
        "        \"operation\": \"divide\",\n",
        "        \"operands\": [a, b],\n",
        "        \"result\": result,\n",
        "        \"expression\": f\"{a} / {b} = {result}\",\n",
        "    }\n",
        "\n",
        "\n",
        "def power(base: float, exponent: float) -> Dict[str, Any]:\n",
        "    \"\"\"Raise a base number to the power of an exponent.\n",
        "\n",
        "    Args:\n",
        "        base: The base number to raise.\n",
        "        exponent: The power to raise the base to.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the operation name, operands, result,\n",
        "        expression string, and an error field if the operation fails.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the operation is mathematically invalid.\n",
        "        OverflowError: If the result is too large to represent.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        result = base**exponent\n",
        "        return {\n",
        "            \"operation\": \"power\",\n",
        "            \"operands\": [base, exponent],\n",
        "            \"result\": result,\n",
        "            \"expression\": f\"{base} ^ {exponent} = {result}\",\n",
        "        }\n",
        "    except (ValueError, OverflowError) as e:\n",
        "        return {\n",
        "            \"operation\": \"power\",\n",
        "            \"operands\": [base, exponent],\n",
        "            \"result\": None,\n",
        "            \"error\": str(e),\n",
        "            \"expression\": f\"{base} ^ {exponent} = ERROR\",\n",
        "        }\n",
        "\n",
        "\n",
        "def square_root(x: float) -> Dict[str, Any]:\n",
        "    \"\"\"Calculate the square root of a number.\n",
        "\n",
        "    Args:\n",
        "        x: The number to calculate the square root for. Must be non-negative.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the operation name, operands, result,\n",
        "        expression string, and an error field if the input is negative.\n",
        "    \"\"\"\n",
        "    if x < 0:\n",
        "        return {\n",
        "            \"operation\": \"square_root\",\n",
        "            \"operands\": [x],\n",
        "            \"result\": None,\n",
        "            \"error\": \"Cannot compute square root of negative number\",\n",
        "            \"expression\": f\"sqrt({x}) = ERROR\",\n",
        "        }\n",
        "    result = x**0.5\n",
        "    return {\n",
        "        \"operation\": \"square_root\",\n",
        "        \"operands\": [x],\n",
        "        \"result\": result,\n",
        "        \"expression\": f\"sqrt({x}) = {result}\",\n",
        "    }\n",
        "\n",
        "\n",
        "def sin(x: float) -> Dict[str, Any]:\n",
        "    \"\"\"Calculate the sine of a number (in radians).\n",
        "\n",
        "    Args:\n",
        "        x: The angle in radians.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the operation name, operands, result, and\n",
        "        expression string.\n",
        "    \"\"\"\n",
        "    import math\n",
        "    result = math.sin(x)\n",
        "    return {\n",
        "        \"operation\": \"sin\",\n",
        "        \"operands\": [x],\n",
        "        \"result\": result,\n",
        "        \"expression\": f\"sin({x}) = {result}\",\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e52b4dc",
      "metadata": {
        "id": "6e52b4dc"
      },
      "source": [
        "## Step 3: Generate JSON Schemas\n",
        "\n",
        "### Understanding JSON Schemas\n",
        "\n",
        "LLMs don't see your Python function code directly. Instead, they work with **JSON schemas** that describe:\n",
        "- Function name\n",
        "- Function description\n",
        "- Parameter names and types\n",
        "- Required parameters\n",
        "\n",
        "The `get_json_schema()` function automatically converts Python functions into the standard JSON schema format.\n",
        "\n",
        "#### Example Schema Structure:\n",
        "```json\n",
        "{\n",
        "  \"type\": \"function\",\n",
        "  \"function\": {\n",
        "    \"name\": \"add\",\n",
        "    \"description\": \"Add two numbers together.\",\n",
        "    \"parameters\": {\n",
        "      \"type\": \"object\",\n",
        "      \"properties\": {\n",
        "        \"a\": {\"type\": \"number\", \"description\": \"The first number to add.\"},\n",
        "        \"b\": {\"type\": \"number\", \"description\": \"The second number to add.\"}\n",
        "      },\n",
        "      \"required\": [\"a\", \"b\"]\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "### Creating the Tools List\n",
        "\n",
        "We convert each calculator function to a JSON schema and store them in a `tools` list. This list will be passed to the model during inference.\n",
        "\n",
        "We also create a `function_map` dictionary to easily execute functions by name when the model requests them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20f27fd1",
      "metadata": {
        "id": "20f27fd1"
      },
      "outputs": [],
      "source": [
        "from collections.abc import Callable\n",
        "\n",
        "\n",
        "tools: List[Dict[str, Dict]] = [\n",
        "    get_json_schema(add),\n",
        "    get_json_schema(subtract),\n",
        "    get_json_schema(multiply),\n",
        "    get_json_schema(divide),\n",
        "    get_json_schema(power),\n",
        "    get_json_schema(square_root),\n",
        "    get_json_schema(sin),\n",
        "]\n",
        "\n",
        "\n",
        "function_map: Dict[str, Callable] = {\n",
        "    \"add\": add,\n",
        "    \"subtract\": subtract,\n",
        "    \"multiply\": multiply,\n",
        "    \"divide\": divide,\n",
        "    \"power\": power,\n",
        "    \"square_root\": square_root,\n",
        "    \"sin\": sin,\n",
        "}\n",
        "\n",
        "print(\"Available Tools:\\n\")\n",
        "print(json.dumps(tools, indent=2))\n",
        "\n",
        "print(function_map[\"multiply\"](2, 3)) # Test call to multiply function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ebfb61d",
      "metadata": {
        "id": "0ebfb61d"
      },
      "source": [
        "**Output**: The `tools` list now contains 8 JSON schemas, and `function_map` allows us to execute functions by name."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57eb6084",
      "metadata": {
        "id": "57eb6084"
      },
      "source": [
        "## Step 4: Parse Tool Calls from Model Output\n",
        "\n",
        "### Why Parsing is Necessary\n",
        "\n",
        "Different models output tool calls in different formats:\n",
        "- Some use XML-like tags: `<tool_call>{...}</tool_call>`\n",
        "- Others use special markers: `[TOOL_CALLS]...[/TOOL_CALLS]`\n",
        "- Some output raw JSON objects\n",
        "\n",
        "### Our Parsing Function\n",
        "\n",
        "The `parse_tool_calls()` function handles multiple formats:\n",
        "\n",
        "1. **XML Format**: `<tool_call>{\"name\": \"add\", \"arguments\": {\"a\": 5, \"b\": 3}}</tool_call>`\n",
        "2. **Raw JSON**: `{\"name\": \"multiply\", \"arguments\": {\"a\": 2, \"b\": 4}}`\n",
        "3. **Array Format**: `[TOOL_CALLS][{...}, {...}][/TOOL_CALLS]`\n",
        "\n",
        "The function uses regular expressions to detect and extract tool call information, making it robust to various model outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3820f4b1",
      "metadata": {
        "id": "3820f4b1"
      },
      "outputs": [],
      "source": [
        "def parse_tool_calls(response_text: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Parse tool calls from model response.\n",
        "    Supports multiple formats:\n",
        "    - <tool_call>{...}</tool_call>\n",
        "    - Multiple individual JSON objects on separate lines\n",
        "    - [TOOL_CALLS] JSON_ARRAY [/TOOL_CALLS]\n",
        "    - Generic JSON objects with \"name\" and \"arguments\"\n",
        "    \"\"\"\n",
        "    tool_calls = []\n",
        "\n",
        "    # Pattern 1: <tool_call> XML format (handles multiline JSON)\n",
        "    try:\n",
        "        # Regex: <tool_call>\\s*(\\{[\\s\\S]*?\\})\\s*</tool_call>\n",
        "        # Matches: <tool_call> + optional whitespace + capture group ({ to }) + optional whitespace + </tool_call>\n",
        "        # [\\s\\S]*? matches any character including newlines (non-greedy)\n",
        "        # This captures JSON objects wrapped in XML-like tags\n",
        "        tool_call_pattern = r\"<tool_call>\\s*(\\{[\\s\\S]*?\\})\\s*</tool_call>\"\n",
        "        tool_call_matches = re.findall(tool_call_pattern, response_text)\n",
        "\n",
        "        if tool_call_matches:\n",
        "            for match in tool_call_matches:\n",
        "                try:\n",
        "                    # Clean up the JSON string\n",
        "                    json_str = match.strip()\n",
        "                    tool_call = json.loads(json_str)\n",
        "                    tool_calls.append(tool_call)\n",
        "                except json.JSONDecodeError as e:\n",
        "                    # Try to find JSON objects within the match\n",
        "                    pass\n",
        "    except Exception as e:\n",
        "        pass\n",
        "\n",
        "    # Pattern 2: Standalone JSON objects (each on own line or separated by newlines)\n",
        "    if not tool_calls:\n",
        "        try:\n",
        "            # Regex: \\{[^{}]*\\\"name\\\"[^{}]*\\\"arguments\\\"[^{}]*\\}\n",
        "            # Matches: { + any chars except braces + \"name\" + any chars except braces + \"arguments\" + any chars except braces + }\n",
        "            # [^{}]* ensures we only match single-level JSON objects (no nested braces)\n",
        "            # This captures flat JSON objects that contain both \"name\" and \"arguments\" keys\n",
        "            json_pattern = r\"\\{[^{}]*\\\"name\\\"[^{}]*\\\"arguments\\\"[^{}]*\\}\"\n",
        "            json_matches = re.findall(json_pattern, response_text)\n",
        "\n",
        "            for match in json_matches:\n",
        "                try:\n",
        "                    tool_call = json.loads(match)\n",
        "                    if \"name\" in tool_call:\n",
        "                        tool_calls.append(tool_call)\n",
        "                except json.JSONDecodeError:\n",
        "                    pass\n",
        "        except Exception as e:\n",
        "            pass\n",
        "\n",
        "    # Pattern 3: [TOOL_CALLS] format\n",
        "    if not tool_calls:\n",
        "        try:\n",
        "            # Regex: \\[TOOL_CALLS\\](.*?)(\\[/TOOL_CALLS\\]|$)\n",
        "            # Matches: [TOOL_CALLS] + capture group (any chars, non-greedy) + either [/TOOL_CALLS] or end of string\n",
        "            # re.DOTALL makes . match newlines too\n",
        "            # (.*?) captures everything between the opening and closing tags (or end of text)\n",
        "            # This handles array format: [TOOL_CALLS][{...}, {...}][/TOOL_CALLS]\n",
        "            match = re.search(\n",
        "                r\"\\[TOOL_CALLS\\](.*?)(\\[/TOOL_CALLS\\]|$)\",\n",
        "                response_text,\n",
        "                re.DOTALL\n",
        "            )\n",
        "            if match:\n",
        "                json_str = match.group(1).strip()\n",
        "                if json_str.startswith(\"[\"):\n",
        "                    tool_calls = json.loads(json_str)\n",
        "                else:\n",
        "                    tool_calls = [json.loads(json_str)]\n",
        "        except Exception as e:\n",
        "            pass\n",
        "\n",
        "    return tool_calls\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8715cdb7",
      "metadata": {
        "id": "8715cdb7"
      },
      "source": [
        "## Step 5: Execute Tool Calls\n",
        "\n",
        "### The Execution Function\n",
        "\n",
        "Once we've parsed the tool call requests from the model, we need to actually execute them. The `execute_tool_call()` function:\n",
        "\n",
        "1. **Validates** the function name exists in our `function_map`\n",
        "2. **Calls** the appropriate function with the provided arguments\n",
        "3. **Handles errors** gracefully with try-catch blocks\n",
        "4. **Returns** the result in a structured format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7bcfbd1",
      "metadata": {
        "id": "e7bcfbd1"
      },
      "outputs": [],
      "source": [
        "def execute_tool_call(func_name: str, args: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"Execute a single tool call.\"\"\"\n",
        "    if func_name not in function_map:\n",
        "        return {\n",
        "            \"error\": f\"Unknown function: {func_name}\",\n",
        "            \"available_functions\": list(function_map.keys())\n",
        "        }\n",
        "\n",
        "    try:\n",
        "        return function_map[func_name](**args)\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"error\": f\"Error executing {func_name}: {str(e)}\",\n",
        "            \"function\": func_name,\n",
        "            \"arguments\": args\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04d0b080",
      "metadata": {
        "id": "04d0b080"
      },
      "source": [
        "## Step 6: Load a Tool-Use-Compatible Model\n",
        "\n",
        "### Model Selection\n",
        "\n",
        "We use **SmolLM3-3B**, a compact language model that supports function calling. Key considerations when choosing a model:\n",
        "\n",
        "- âœ… **Tool-use capability**: The model must be trained to understand and generate tool calls\n",
        "- âœ… **Size vs Performance**: Larger models (e.g., Llama-3-70B, Command-R) offer better performance but require more resources\n",
        "- âœ… **Format compatibility**: Different models use different tool call formats, e.g.,  <tool_call> XML format, Standalone JSON objects, [TOOL_CALLS] format\n",
        "\n",
        "### Popular Tool-Use Models\n",
        "\n",
        "- **SmolLM3** (3B parameters): Lightweight, good for experimentation\n",
        "- **Hermes-2-Pro-Llama-3-8B**: Strong tool-use performance for its size\n",
        "- **Command-R**: Enterprise-grade performance with RAG support\n",
        "- **Mixtral-8x22B**: High performance, supports multiple simultaneous tool calls\n",
        "- **GPT-4 / Claude**: Commercial APIs with robust tool support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ed2847c",
      "metadata": {
        "id": "9ed2847c"
      },
      "outputs": [],
      "source": [
        "# Load model\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "\n",
        "# https://huggingface.co/HuggingFaceTB/SmolLM3-3B\n",
        "MODEL_NAME = \"HuggingFaceTB/SmolLM3-3B\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    use_fast=True,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "model.eval()\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(f\"Model loaded successfully on device: {model.device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faf2fb58",
      "metadata": {
        "id": "faf2fb58"
      },
      "source": [
        "## Step 7: First Tool Call - Initial Model Response\n",
        "\n",
        "### The Process Flow\n",
        "\n",
        "1. **Create conversation**: Build a messages list with system prompt and user query\n",
        "2. **Apply chat template**: Use `apply_chat_template()` with `tools` parameter\n",
        "3. **Generate response**: Let the model decide which tools to call\n",
        "4. **Decode output**: Convert model tokens to text\n",
        "\n",
        "### Key Parameters\n",
        "\n",
        "- `tools=tools`: Passes our function schemas to the model\n",
        "- `add_generation_prompt=True`: Adds the assistant's turn marker\n",
        "- `return_dict=True`: Returns input_ids and attention_mask\n",
        "- `enable_thinking=False`: Disables chain-of-thought (optional feature)\n",
        "\n",
        "### What to Expect\n",
        "\n",
        "The model will analyze the query: *\"What is the result of (15 + 27) * 3 - sqrt(81)?\"*\n",
        "\n",
        "It should recognize that it needs to:\n",
        "1. First add 15 + 27\n",
        "2. Then multiply the result by 3\n",
        "3. Calculate the square root of 81\n",
        "4. Finally subtract the square root from the multiplication result\n",
        "\n",
        "**Output**: **SmolLM3-3B** defaults to tool call requests in XML format. You should see `<tool_call>...</tool_call>` tags wrapping a JSON schema specifying which functions to call and their arguments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc3cb69b",
      "metadata": {
        "id": "bc3cb69b"
      },
      "outputs": [],
      "source": [
        "# Define the user's mathematical query\n",
        "user_query = \"What is the result of 3.8^2.2 * sin(3)?\"\n",
        "\n",
        "# Build the conversation with system prompt and user query\n",
        "# This creates the initial context for the model\n",
        "conversation_messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful calculator assistant. Use the provided calculator functions to solve math problems accurately. CRITICAL: You must NEVER compute any (even the simplist) mathematical results yourself. ALWAYS call the appropriate functions and WAIT for their computed results, excluding nested calls as we are computing iteratively. Do not provide any numerical answers until you have received the tool's output.\",\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": user_query},\n",
        "    ]\n",
        "\n",
        "# Apply chat template to format the conversation for the model\n",
        "# tools=tools: Include function schemas so model knows what functions are available\n",
        "# add_generation_prompt=True: Add the assistant's turn marker\n",
        "# return_dict=True: Return both input_ids and attention_mask\n",
        "# return_tensors=\"pt\": Return PyTorch tensors\n",
        "# enable_thinking=False: Disable chain-of-thought reasoning\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    conversation_messages,\n",
        "    tools=tools,\n",
        "    add_generation_prompt=True,\n",
        "    return_dict=True,\n",
        "    return_tensors=\"pt\",\n",
        "    enable_thinking=False,\n",
        ").to(model.device)\n",
        "\n",
        "# Generate the model's response\n",
        "# The model will analyze the query and decide which tools to call\n",
        "# max_new_tokens=256: Generate up to 256 new tokens\n",
        "# temperature=0.6: Controls randomness (lower = more deterministic)\n",
        "# do_sample=True: Enable sampling for more varied responses\n",
        "# top_p=0.95: Nucleus sampling - consider tokens with cumulative probability 95%\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=1024,\n",
        "    temperature=0.2,\n",
        "    do_sample=True,\n",
        "    top_p=0.95,\n",
        ")\n",
        "\n",
        "# outputs[0][inputs[\"input_ids\"].shape[1]:] extracts just the new tokens\n",
        "# skip_special_tokens=False: Keep special tokens like <tool_call> tags for parsing\n",
        "out_text = tokenizer.decode(\n",
        "    outputs[0][inputs[\"input_ids\"].shape[1] :], skip_special_tokens=False\n",
        ")\n",
        "\n",
        "print(\"Initial model output:\\n\", out_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfcdd32b",
      "metadata": {
        "id": "dfcdd32b"
      },
      "source": [
        "## Step 8: Parse Model Output's Tool Call Requests\n",
        "\n",
        "Now we extract the structured tool call requests from the model's output using the `parse_tool_calls()` function.\n",
        "\n",
        "### Expected Output Format\n",
        "\n",
        "Each parsed tool call should contain:\n",
        "```python\n",
        "{\n",
        "    \"name\": \"function_name\",\n",
        "    \"arguments\": {\"arg1\": value1, \"arg2\": value2},\n",
        "    \"id\": \"call_id\"  # Optional, used by some models\n",
        "}\n",
        "```\n",
        "\n",
        "For our example query, we expect to see calls to functions like `add`, `multiply`, and `square_root`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5a8604d",
      "metadata": {
        "id": "a5a8604d"
      },
      "outputs": [],
      "source": [
        "tool_calls_parsed = parse_tool_calls(out_text)\n",
        "if tool_calls_parsed:\n",
        "    print(\"First tool calls found:\", tool_calls_parsed)\n",
        "else:\n",
        "    print(\"No tool calls parsed from the model output.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c001cd37",
      "metadata": {
        "id": "c001cd37"
      },
      "source": [
        "## Step 9: Add Tool Calls to Conversation\n",
        "\n",
        "### Conversation Format\n",
        "\n",
        "We need to append the assistant's tool call requests to the conversation history. The standard format is:\n",
        "\n",
        "```python\n",
        "{\n",
        "    \"role\": \"assistant\",\n",
        "    \"tool_calls\": [\n",
        "        {\n",
        "            \"type\": \"function\",\n",
        "            \"function\": {\n",
        "                \"name\": \"function_name\",\n",
        "                \"arguments\": {...}\n",
        "            }\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "```\n",
        "\n",
        "### Why This Matters\n",
        "\n",
        "This standardized format:\n",
        "- Maintains conversation continuity\n",
        "- Allows the model to track its own tool calls\n",
        "- Enables proper context for interpreting tool results\n",
        "- Follows the OpenAI/HuggingFace API conventions (with dict arguments, not JSON strings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96890c2c",
      "metadata": {
        "id": "96890c2c"
      },
      "outputs": [],
      "source": [
        "tool_calls = [{\"type\": \"function\", \"function\": f} for f in tool_calls_parsed]\n",
        "\n",
        "conversation_messages.append({\"role\": \"assistant\", \"tool_calls\": tool_calls})\n",
        "\n",
        "\n",
        "print(\"Conversation messages with tool calls:\", conversation_messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bdd68cc",
      "metadata": {
        "id": "5bdd68cc"
      },
      "source": [
        "**Output**: The conversation now includes an assistant message with properly formatted tool calls."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2046531f",
      "metadata": {
        "id": "2046531f"
      },
      "source": [
        "## Step 10: Execute Functions and Add Results\n",
        "\n",
        "### The Execution Loop\n",
        "\n",
        "For each tool call requested by the model:\n",
        "1. Extract the function name and arguments\n",
        "2. Call `execute_tool_call()` to run the actual function\n",
        "3. Convert the result to JSON string\n",
        "4. Append a tool message to the conversation\n",
        "\n",
        "### Tool Message Format\n",
        "\n",
        "```python\n",
        "{\n",
        "    \"role\": \"tool\",\n",
        "    \"name\": \"function_name\",\n",
        "    \"content\": \"json_result_string\",\n",
        "    \"tool_call_id\": \"call_id\"  # Required by some models like Mistral/Mixtral\n",
        "}\n",
        "```\n",
        "\n",
        "### Important Notes\n",
        "\n",
        "- The `content` must be a **string**, not a dict (convert results with `json.dumps()`)\n",
        "- Some models require a `tool_call_id` to match calls with responses\n",
        "- The `role: \"tool\"` indicates this message contains function output\n",
        "\n",
        "This step is where the actual computation happens - the LLM's requests are translated into real function executions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d440cc77",
      "metadata": {
        "id": "d440cc77"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "for i, tool_call in enumerate(tool_calls_parsed):\n",
        "    func_name = tool_call.get(\"name\")\n",
        "    args = tool_call.get(\"arguments\", {})\n",
        "    call_id = tool_call.get(\"id\", f\"call_{i}\")\n",
        "\n",
        "\n",
        "    print(f\"\\n  [{i+1}] Executing: {func_name}({args})\")\n",
        "\n",
        "    result = execute_tool_call(func_name, args)\n",
        "\n",
        "    if \"error\" in result:\n",
        "      print (f\"{result}, skipping appending it to the conversation history.\")\n",
        "      continue\n",
        "\n",
        "    print(f\"Result: {result}\")\n",
        "\n",
        "    conversation_messages.append(\n",
        "        {\n",
        "            \"role\": \"tool\",\n",
        "            \"name\": func_name,\n",
        "            \"content\": json.dumps(result),\n",
        "            \"tool_call_id\": call_id,\n",
        "        }\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56d130c0",
      "metadata": {
        "id": "56d130c0"
      },
      "source": [
        "**Output**: Each tool call is executed and you'll see the function being called with its arguments, followed by the result (e.g., `power(3.8, 2.2)` â†’ `{\"operation\": \"power\", ..., \"result\": 18.859, ...}`)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80156af4",
      "metadata": {
        "id": "80156af4"
      },
      "source": [
        "## Step 11: Review the Complete Conversation\n",
        "\n",
        "At this point, our conversation history contains:\n",
        "1. **System message**: Instructions for the assistant\n",
        "2. **User message**: The original query\n",
        "3. **Assistant message**: Tool call requests\n",
        "4. **Tool message(s)**: Function execution results\n",
        "\n",
        "This complete context will be passed to the model in the next generation step, allowing it to formulate a final answer based on the tool outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "971e54c8",
      "metadata": {
        "id": "971e54c8"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"Conversation messages after tool execution:\", conversation_messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3658431",
      "metadata": {
        "id": "a3658431"
      },
      "source": [
        "**Output**: The complete conversation history showing all messages: system, user, assistant (with tool calls), and tool (with results)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e6db9c1",
      "metadata": {
        "id": "6e6db9c1"
      },
      "source": [
        "## Step 12: Generate Second-Round Response\n",
        "\n",
        "Now we pass the **entire conversation** (including tool results) back to the model to generate a natural language response.\n",
        "\n",
        "### What Happens Here\n",
        "\n",
        "1. The model reads its own tool call requests\n",
        "2. It processes the tool execution results\n",
        "3. It synthesizes the information into a user-friendly response with next-step planning.\n",
        "\n",
        "### Expected Output\n",
        "\n",
        "For the query *\"What is the result of 3.8^2.2 * sin(3)?\"*, the model should incorporate the calculated results of 3.8^2.2 = 18.859 and sin(3) = 0.141, and plan for next tool call to calculate 18.859 * 0.141."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c4e613e",
      "metadata": {
        "id": "6c4e613e"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer.apply_chat_template(\n",
        "    conversation_messages,\n",
        "    tools=tools,\n",
        "    add_generation_prompt=True,\n",
        "    return_dict=True,\n",
        "    return_tensors=\"pt\",\n",
        "    enable_thinking=False,\n",
        ").to(model.device)\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.1,\n",
        "    do_sample=False,\n",
        "    top_p=0.95,\n",
        ")\n",
        "\n",
        "model_response = tokenizer.decode(\n",
        "    outputs[0][inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "print(\"User query:\\n\", user_query)\n",
        "print(\"\\nModel response:\\n\", model_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16aa6cf7",
      "metadata": {
        "id": "16aa6cf7"
      },
      "source": [
        "**Output**: (Hopefully) another tool call requesting multiplying the result of 3.8^2.2 with the result of sin(3)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 13: Full with Multi-Turn Tool Use Pipeline\n",
        "\n",
        "### Building a Robust Pipeline\n",
        "\n",
        "The previous steps showed a full tool-use round + begin of the second round. In practice, complex queries may require **multiple rounds** of tool calls:\n",
        "\n",
        "Example: *\"What is ((5 + 3) * 2) ^ 2?\"*\n",
        "- **Round 1**: Call `add(5, 3)` â†’ Get result: 8\n",
        "- **Round 2**: Call `multiply(8, 2)` â†’ Get result: 16\n",
        "- **Round 3**: Call `power(16, 2)` â†’ Get result: 256\n",
        "- **Final**: Generate answer: \"256\"\n",
        "\n",
        "### Pipeline Features\n",
        "\n",
        "The `pipeline()` function handles:\n",
        "\n",
        "1. **Multi-turn conversations**: Automatically loops until no more tool calls\n",
        "2. **Tool call limits**: Prevents infinite loops with `max_tool_calls`\n",
        "3. **Automatic result injection**: Executes functions and adds results to conversation\n",
        "4. **Thinking mode**: Optional chain-of-thought reasoning"
      ],
      "metadata": {
        "id": "BavXiyq7kmzy"
      },
      "id": "BavXiyq7kmzy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d60ee7e1",
      "metadata": {
        "id": "d60ee7e1"
      },
      "outputs": [],
      "source": [
        "user_query = \"What is the result of 3.8^2.2 * sin(3)?\"\n",
        "print(\"User query:\\n\", user_query)\n",
        "\n",
        "def pipeline(\n",
        "    user_query: str,\n",
        "    thinking: bool = False,\n",
        "    max_new_tokens: int = 512,\n",
        "    max_tool_calls: int = 5,\n",
        ") -> str:\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful calculator assistant. Use the provided calculator functions to solve math problems accurately. CRITICAL: You must NEVER compute any (even the simplist) mathematical results yourself. ALWAYS call the appropriate functions and WAIT for their computed results, excluding nested calls as we are computing iteratively. Do not provide any numerical answers until you have received the tool's output.\",\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": user_query},\n",
        "    ]\n",
        "\n",
        "    round_idx = 0\n",
        "    while round_idx < max_tool_calls:\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"ROUND {round_idx + 1}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        inputs = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tools=tools,\n",
        "            add_generation_prompt=True,\n",
        "            return_dict=True,\n",
        "            return_tensors=\"pt\",\n",
        "            enable_thinking=thinking,\n",
        "        ).to(model.device)\n",
        "\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.2,\n",
        "            do_sample=False,\n",
        "            top_p=0.95,\n",
        "        )\n",
        "\n",
        "        out_text = tokenizer.decode(\n",
        "            outputs[0][inputs[\"input_ids\"].shape[1] :], skip_special_tokens=False\n",
        "        )\n",
        "\n",
        "        tool_calls_parsed = parse_tool_calls(out_text)\n",
        "\n",
        "        if not tool_calls_parsed:\n",
        "            print(\"\\nâœ“ No more tool calls detected - generating final answer...\")\n",
        "            print(f\"\\nFinal Response:\\n{out_text}\")\n",
        "            return out_text\n",
        "\n",
        "        if round_idx == max_tool_calls - 1:\n",
        "            print(f\"\\nâš  Maximum tool call rounds ({max_tool_calls}) reached.\")\n",
        "            return out_text\n",
        "\n",
        "        print(f\"\\nðŸ“‹ Tool Calls Requested: {len(tool_calls_parsed)}\")\n",
        "\n",
        "        tool_calls = [{\"type\": \"function\", \"function\": f} for f in tool_calls_parsed]\n",
        "        messages.append({\"role\": \"assistant\", \"tool_calls\": tool_calls})\n",
        "\n",
        "        for i, tool_call in enumerate(tool_calls_parsed):\n",
        "            func_name = tool_call.get(\"name\")\n",
        "            args = tool_call.get(\"arguments\", {})\n",
        "\n",
        "            print(f\"\\n  ðŸ”§ [{i+1}] Calling: {func_name}({', '.join(f'{k}={v}' for k, v in args.items())})\")\n",
        "\n",
        "            result = execute_tool_call(func_name, args)\n",
        "\n",
        "            if \"error\" in result:\n",
        "                print(f\"     âŒ Error: {result['error']}\")\n",
        "                # Skip adding errored results to conversation\n",
        "                continue\n",
        "            elif \"result\" in result:\n",
        "                print(f\"     âœ“ Result: {result['result']}\")\n",
        "            else:\n",
        "                print(f\"     âœ“ Result: {result}\")\n",
        "\n",
        "            tool_messsage = {\n",
        "                \"role\": \"tool\",\n",
        "                \"name\": func_name,\n",
        "                \"content\": json.dumps(result),\n",
        "            }\n",
        "\n",
        "            if \"id\" in tool_call:\n",
        "                tool_messsage[\"tool_call_id\"] = tool_call[\"id\"]\n",
        "\n",
        "            messages.append(tool_messsage)\n",
        "\n",
        "        round_idx += 1\n",
        "\n",
        "\n",
        "model_response = pipeline(user_query, max_new_tokens=512)\n",
        "print(\"\\nModel response:\\n\", model_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f0885d0",
      "metadata": {
        "id": "2f0885d0"
      },
      "source": [
        "**Output**: You'll see round-by-round progress (e.g., \"--- Round 1 ---\", \"--- Round 2 ---\") as the model makes tool calls, receives results, and eventually generates the final answer."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c54213f",
      "metadata": {
        "id": "7c54213f"
      },
      "source": [
        "---\n",
        "\n",
        "## Summary and Key Takeaways\n",
        "\n",
        "### What We Learned\n",
        "\n",
        "1. **Function Calling Basics**\n",
        "   - LLMs can request external function calls rather than computing everything internally\n",
        "   - Functions must follow specific conventions (type hints, docstrings)\n",
        "   - JSON schemas describe functions to the model\n",
        "\n",
        "2. **The Tool-Use Workflow**\n",
        "   ```\n",
        "   User Query â†’ Model Analysis â†’ Tool Call Request â†’\n",
        "   Function Execution â†’ Result Injection â†’ Final Response\n",
        "   ```\n",
        "\n",
        "3. **Critical Components**\n",
        "   - **Tool definitions**: Well-documented Python functions\n",
        "   - **Schema generation**: Converting functions to JSON\n",
        "   - **Parsing**: Extracting tool calls from model output\n",
        "   - **Execution**: Running the requested functions\n",
        "   - **Result formatting**: Feeding results back in the correct format\n",
        "\n",
        "4. **Multi-Turn Interactions**\n",
        "   - Complex queries may require multiple rounds\n",
        "   - Each round builds on previous results\n",
        "   - Proper conversation management is essential\n",
        "\n",
        "\n",
        "### Real-World Applications\n",
        "\n",
        "Function calling enables:\n",
        "- **Weather bots**: Access real-time weather data\n",
        "- **Data analysis**: Query databases and visualize results\n",
        "- **Calculators**: Perform precise mathematical operations\n",
        "- **Search agents**: Retrieve information from external sources\n",
        "- **E-commerce**: Check inventory, place orders\n",
        "- **Scheduling**: Book appointments, check availability\n",
        "- **API integration**: Interact with any REST API"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c6ad02e",
      "metadata": {
        "id": "5c6ad02e"
      },
      "source": [
        "## References and Further Reading\n",
        "\n",
        "### Official Documentation\n",
        "\n",
        "**Hugging Face Transformers - Tool Use**\n",
        "- [Expanding Chat Templates with Tools and Documents](https://huggingface.co/docs/transformers/v4.49.0/en/chat_template_tools_and_documents)\n",
        "- [Tool Use Guide (v5.0)](https://huggingface.co/docs/transformers/v5.0.0rc2/chat_extras)\n",
        "\n",
        "### Recommended Models for Tool Use\n",
        "\n",
        "**Open-Source Models**\n",
        "- [Hermes-2-Pro-Llama-3-8B](https://huggingface.co/NousResearch/Hermes-2-Pro-Llama-3-8B) - Excellent tool-use performance\n",
        "- [SmolLM3-3B](https://huggingface.co/HuggingFaceTB/SmolLM3-3B) - Lightweight, good for learning\n",
        "- [Command-R](https://huggingface.co/CohereForAI/c4ai-command-r-v01) - Enterprise-grade with RAG support\n",
        "- [Mixtral-8x22B](https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1) - High performance, parallel tool calls\n",
        "- [Llama-3.1 series](https://huggingface.co/meta-llama) - Strong tool-use capabilities\n",
        "\n",
        "### Additional Resources\n",
        "\n",
        "- [JSON Schema Documentation](https://json-schema.org/learn/getting-started-step-by-step)\n",
        "- [Google Python Style Guide - Docstrings](https://google.github.io/styleguide/pyguide.html#38-comments-and-docstrings)\n",
        "- [OpenAI Function Calling Guide](https://platform.openai.com/docs/guides/function-calling)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}