{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6989fd99",
   "metadata": {},
   "source": [
    "# Model Quantization with HuggingFace Transformers\n",
    "\n",
    "Quantization reduces memory usage and can improve throughput at a modest accuracy cost.\n",
    "\n",
    "## Why Quantization?\n",
    "- **Memory**: 4-bit models use ~1/4 the memory of fp16 models\n",
    "- **Speed**: Lower precision can enable faster inference\n",
    "- **Deployment**: Fit larger models on smaller GPUs\n",
    "\n",
    "## Quantization Methods:\n",
    "- **8-bit (INT8)**: Good balance, minimal accuracy loss\n",
    "- **4-bit (NF4)**: Maximum compression, some quality tradeoff\n",
    "- **GPTQ/AWQ**: Post-training quantization optimized for inference\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eca862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up torch for optimal performance\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.fp32_precision = \"tf32\"\n",
    "    torch.backends.cudnn.conv.fp32_precision = \"tf32\"\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Model identifiers\n",
    "MODEL_INSTRUCT = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "MODEL_THINKING = \"Qwen/Qwen3-4B-Thinking-2507\"\n",
    "\n",
    "def report_memory(tag):\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"CUDA not available.\")\n",
    "        return\n",
    "    torch.cuda.synchronize()\n",
    "    allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    print(f\"{tag}: allocated {allocated:.2f} GB, reserved {reserved:.2f} GB\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "report_memory(\"Initial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17af864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "def load_model(model_id, quantization_config=None):\n",
    "    \"\"\"Load a model and tokenizer with optional quantization.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_id,\n",
    "        use_fast=True,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        dtype=torch.bfloat16,\n",
    "        quantization_config=quantization_config,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    model.eval()\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    return tokenizer, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a9799c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_messages(user_prompt, system_prompt=\"You are a helpful assistant.\"):\n",
    "    \"\"\"Format messages for chat models.\"\"\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "\n",
    "def generate_chat(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"Generate a chat response.\"\"\"\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    gen_kwargs = dict(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        **kwargs,\n",
    "    )\n",
    "    if do_sample:\n",
    "        gen_kwargs.update({\"temperature\": temperature, \"top_p\": top_p})\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(**gen_kwargs)\n",
    "\n",
    "    gen_ids = output_ids[0, input_ids.shape[-1] :]\n",
    "    return tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "comparison_prompts = {\n",
    "    \"math\": \"Solve: If a train travels 120 km in 1.5 hours, what is its average speed?\",\n",
    "    \"reasoning\": \"You have 3 boxes: apples, oranges, and mixed. All labels are wrong. \"\n",
    "    \"Pick one fruit to identify all boxes. Explain.\",\n",
    "    \"analysis\": \"Compare pros and cons of deploying an LLM on-prem vs in the cloud.\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72ff2ff",
   "metadata": {},
   "source": [
    "## Three-Way Comparison: Baseline vs INT8 vs NF4\n",
    "\n",
    "Run the same test prompts across all three quantization levels to compare outputs and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53156f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store results from all three quantization methods\n",
    "all_results = {}\n",
    "\n",
    "# 1. BASELINE (Full Precision)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LOADING: Full Precision (BF16)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'model' in locals() and model is not None:\n",
    "    del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "tokenizer, model = load_model(MODEL_INSTRUCT)\n",
    "report_memory(\"Baseline (FP16/BF16)\")\n",
    "\n",
    "baseline_results = {}\n",
    "for name, prompt in comparison_prompts.items():\n",
    "    messages = format_messages(prompt, system_prompt=\"You are a precise assistant.\")\n",
    "    baseline_results[name] = generate_chat(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        messages,\n",
    "        max_new_tokens=200,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "all_results['Baseline (FP16)'] = baseline_results\n",
    "print(\"✓ Baseline results collected.\\n\")\n",
    "\n",
    "# 2. INT8 QUANTIZATION\n",
    "print(\"=\"*60)\n",
    "print(\"LOADING: 8-bit Quantization (INT8)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'model' in locals() and model is not None:\n",
    "    del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "bnb_8bit = BitsAndBytesConfig(load_in_8bit=True)\n",
    "tokenizer, model = load_model(MODEL_INSTRUCT, quantization_config=bnb_8bit)\n",
    "report_memory(\"INT8 (8-bit)\")\n",
    "\n",
    "int8_results = {}\n",
    "for name, prompt in comparison_prompts.items():\n",
    "    messages = format_messages(prompt, system_prompt=\"You are a precise assistant.\")\n",
    "    int8_results[name] = generate_chat(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        messages,\n",
    "        max_new_tokens=200,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "all_results['INT8 (8-bit)'] = int8_results\n",
    "print(\"✓ INT8 results collected.\\n\")\n",
    "\n",
    "# 3. NF4 QUANTIZATION\n",
    "print(\"=\"*60)\n",
    "print(\"LOADING: 4-bit Quantization (NF4)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'model' in locals() and model is not None:\n",
    "    del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "bnb_4bit = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer, model = load_model(MODEL_INSTRUCT, quantization_config=bnb_4bit)\n",
    "report_memory(\"NF4 (4-bit)\")\n",
    "\n",
    "nf4_results = {}\n",
    "for name, prompt in comparison_prompts.items():\n",
    "    messages = format_messages(prompt, system_prompt=\"You are a precise assistant.\")\n",
    "    nf4_results[name] = generate_chat(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        messages,\n",
    "        max_new_tokens=200,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "all_results['NF4 (4-bit)'] = nf4_results\n",
    "print(\"✓ NF4 results collected.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c309520",
   "metadata": {},
   "source": [
    "### Display Results\n",
    "\n",
    "Compare outputs across all three quantization levels side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a0d092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comprehensive three-way comparison\n",
    "for task_name in comparison_prompts:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TASK: {task_name.upper()}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    for quant_type, results in all_results.items():\n",
    "        print(f\"\\n[{quant_type}]\")\n",
    "        print(\"-\" * 70)\n",
    "        print(results[task_name])\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a493e7a",
   "metadata": {},
   "source": [
    "### Memory Usage Summary\n",
    "\n",
    "Review the memory statistics printed during model loading:\n",
    "- **Baseline (BF16)**: Highest accuracy, maximum memory usage\n",
    "- **INT8**: ~50% memory reduction with minimal quality loss\n",
    "- **NF4 (4-bit)**: ~75% memory reduction with acceptable quality tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e269ab",
   "metadata": {},
   "source": [
    "## Other Quantization Formats\n",
    "\n",
    "Beyond bitsandbytes, several other quantization methods are available:\n",
    "\n",
    "#### GPTQ (GPU Post-Training Quantization)\n",
    "- Pre-quantized models available on HuggingFace Hub\n",
    "- Optimized for fast inference on GPU\n",
    "- Loading: Use `AutoGPTQForCausalLM` class or models with `-GPTQ` suffix\n",
    "\n",
    "#### AWQ (Activation-Aware Weight Quantization)\n",
    "- Preserves important weights based on activation magnitudes\n",
    "- Better accuracy than naive 4-bit quantization\n",
    "- Loading: Use `AutoAWQForCausalLM` class or models with `-AWQ` suffix\n",
    "\n",
    "#### GGUF (llama.cpp format)\n",
    "- Optimized for CPU inference\n",
    "- Various quantization levels (Q4_K_M, Q5_K_S, Q8_0, etc.)\n",
    "- Use with `llama-cpp-python` or similar libraries\n",
    "\n",
    "**Example Hub Search**:\n",
    "- GPTQ models: Search for \"gptq\" in model names\n",
    "- AWQ models: Search for \"awq\" in model names\n",
    "\n",
    "Refer to model cards on HuggingFace Hub for supported quantization variants and detailed loading instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230a5c07",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "**When to Use Each Quantization Method:**\n",
    "\n",
    "1. **Full Precision (BF16/FP16)**: \n",
    "   - When you have sufficient GPU memory\n",
    "   - Maximum accuracy is critical\n",
    "   - Baseline for comparison\n",
    "\n",
    "2. **8-bit (INT8)**:\n",
    "   - Good balance between memory and quality\n",
    "   - Minimal accuracy degradation\n",
    "   - ~50% memory savings\n",
    "\n",
    "3. **4-bit (NF4)**:\n",
    "   - Maximum memory efficiency\n",
    "   - Acceptable quality for most tasks\n",
    "   - ~75% memory savings\n",
    "   - Enables larger models on smaller GPUs\n",
    "\n",
    "4. **GPTQ/AWQ**:\n",
    "   - Pre-quantized models for production deployment\n",
    "   - Fast inference with optimized kernels\n",
    "   - Available on HuggingFace Hub\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
